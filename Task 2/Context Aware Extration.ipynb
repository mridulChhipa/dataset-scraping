{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2212398c-c421-40f2-addb-1c8d725b0ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mChecking connectivity to the model hosters, this may take a while. To bypass this check, set `DISABLE_MODEL_SOURCE_CHECK` to `True`.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field, ConfigDict\n",
    "from pdf2image import convert_from_path\n",
    "from paddleocr import PaddleOCR\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c5d0d99-1edd-4c7b-a759-0614e08af9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "BASE_DIR = r\"D:\\mridul\\Scraping Assessment\\Task 2\"\n",
    "POPPLER_BIN = os.path.join(BASE_DIR, \"poppler\", \"Library\", \"bin\")\n",
    "PDF_PATH = os.path.join(BASE_DIR, \"Service Record.pdf\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"output\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b19c6d66-5e5a-476b-817d-8023af13181a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mCreating model: ('PP-OCRv5_server_det', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `C:\\Users\\mridu\\.paddlex\\official_models\\PP-OCRv5_server_det`.\u001b[0m\n",
      "D:\\envs\\ocr-env\\Lib\\site-packages\\paddle\\utils\\cpp_extension\\extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "\u001b[32mCreating model: ('PP-OCRv5_server_rec', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `C:\\Users\\mridu\\.paddlex\\official_models\\PP-OCRv5_server_rec`.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ocr_engine = PaddleOCR(use_doc_orientation_classify=False, use_doc_unwarping=False, \n",
    "                       use_textline_orientation=False, device=\"gpu\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a90b96c5-22e8-473d-af35-aed6082c579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ServiceEntry(BaseModel):\n",
    "    # Mapping Pydantic fields to your exact required CSV headers\n",
    "    Full_Name: str = Field(alias=\"Full Name\")\n",
    "    Educational_Qualification: Optional[str] = Field(alias=\"Educational Qualification\")\n",
    "    Honorific_Title: Optional[str] = Field(alias=\"Honorific/Title\")\n",
    "    Date_of_Birth: str = Field(alias=\"Date of Birth\")\n",
    "    Date_of_Joining_Service: str = Field(alias=\"Date of Joining Service\")\n",
    "    Date_of_Arrival: str = Field(alias=\"Date of Arrival\")\n",
    "    Voted_Non_voted: str = Field(alias=\"Voted/Non-voted\")\n",
    "    Domicile: str = Field(alias=\"Domicile\")\n",
    "    Station: str = Field(alias=\"Station\")\n",
    "    Substantive_Appointment: str = Field(alias=\"Substantive Appointment\")\n",
    "    Subst_Date: str = Field(alias=\"Subst. Date\")\n",
    "    Officiating_Appointment: Optional[str] = Field(alias=\"Officiating Appointment\")\n",
    "    Off_Date: Optional[str] = Field(alias=\"Off. Date\")\n",
    "\n",
    "    model_config = ConfigDict(populate_by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9538f17e-8c92-45a8-ba10-eb99851d7d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ServiceRecordDataset(BaseModel):\n",
    "    entries: List[ServiceEntry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ea40e09-2547-417b-b575-2d3ac3e083f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm = llm.with_structured_output(ServiceRecordDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e51f1512-a28d-4993-bf1c-18530fd9726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_ocr_text(img_path):\n",
    "    \"\"\"Executes PaddleOCR 3.x and extracts text based on detected rec_texts key.\"\"\"\n",
    "    result = ocr_engine.predict(img_path)\n",
    "    raw_text_lines = []\n",
    "    \n",
    "    for res in result:\n",
    "        # Based on your debug log, text is in the 'rec_texts' list\n",
    "        if 'rec_texts' in res and isinstance(res['rec_texts'], list):\n",
    "            raw_text_lines.extend(res['rec_texts'])\n",
    "        \n",
    "        # Optional: Save debug output to folder as requested earlier\n",
    "        res.save_to_json(OUTPUT_DIR)\n",
    "            \n",
    "    raw_text = \"\\n\".join(raw_text_lines)\n",
    "    print(f\"Extracted {len(raw_text_lines)} lines of text.\")\n",
    "    return raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e78e766-4218-4d01-8aad-fc11e61ce7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_page(page_index, previous_context=None):\n",
    "    \"\"\"\n",
    "    Handles extraction for one page, accepting context from the previous page\n",
    "    to handle records that span across page breaks.\n",
    "    \"\"\"\n",
    "    images = convert_from_path(PDF_PATH, dpi=300, first_page=page_index+1, \n",
    "                               last_page=page_index+1, poppler_path=POPPLER_BIN)\n",
    "    if not images: return []\n",
    "    \n",
    "    img_path = os.path.join(OUTPUT_DIR, f\"temp_p{page_index}.png\")\n",
    "    images[0].save(img_path)\n",
    "    \n",
    "    raw_text = get_page_ocr_text(img_path)\n",
    "    \n",
    "    with open(img_path, \"rb\") as f:\n",
    "        img_base64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "    # Format the context string if it exists\n",
    "    # Inside process_single_page function:\n",
    "\n",
    "    context_str = \"START OF NEW RECORD: No previous context.\"\n",
    "    if previous_context:\n",
    "        # Build a comprehensive identity signature from the previous page's last row\n",
    "        context_str = (\n",
    "            f\"CONTINUING RECORD: The current officer is '{previous_context.get('Full Name')}'.\\n\"\n",
    "            f\"- Education: '{previous_context.get('Educational Qualification')}'\\n\"\n",
    "            f\"- Title: '{previous_context.get('Honorific/Title')}'\\n\"\n",
    "            f\"- DOB: '{previous_context.get('Date of Birth')}'\\n\"\n",
    "            f\"- Joining Date: '{previous_context.get('Date of Joining Service')}'\\n\"\n",
    "            f\"- Arrival Date: '{previous_context.get('Date of Arrival')}'\\n\"\n",
    "            f\"- Voted Status: '{previous_context.get('Voted/Non-voted')}'\\n\"\n",
    "            f\"- Domicile: '{previous_context.get('Domicile')}'\"\n",
    "        )\n",
    "        \n",
    "    prompt = f\"\"\"\n",
    "    TASK: Extract service record entries into a structured dataset.\n",
    "    \n",
    "    IMPORTANT - PAGE CONTINUATION CONTEXT:\n",
    "    {context_str}\n",
    "\n",
    "    EXTRACTION INSTRUCTIONS FOR PAGE BREAKS:\n",
    "    1. If this page starts with table rows without a new name header, you MUST use the \n",
    "       'CONTINUING RECORD' details provided above for every row.\n",
    "    2. Fill 'Educational Qualification', 'Honorific/Title', 'Domicile', and 'Voted/Non-voted' \n",
    "       using the context above for all continued rows.\n",
    "    3. Do NOT leave these fields empty for the continuation rows (38 to 45 in your current view).\n",
    "    4. Only change these values if a clear NEW officer header (different name) appears on the page.\n",
    "\n",
    "    MANDATORY RULES:\n",
    "    - Replace 'Do.', '..', or '\"' with the value from the row directly above it.\n",
    "    - Normalize all dates to DD-MM-YYYY (e.g., 8-4-80 -> 08-04-1880).\n",
    "    - Ensure 'Full Name' is present in every single row.\n",
    "\n",
    "    # OCR TEXT FOR SPATIAL REFERENCE:\n",
    "    # {raw_text}\n",
    "    \"\"\"\n",
    "    message = HumanMessage(content=[\n",
    "        {\"type\": \"text\", \"text\": prompt}\n",
    "        # {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"}}\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        response = structured_llm.invoke([message])\n",
    "        return [e.model_dump(by_alias=True) for e in response.entries]\n",
    "    except Exception as e:\n",
    "        print(f\"Error on page {page_index+1}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37024cf9-91e6-4e59-aea9-3a960eac3779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_extraction(start_p, end_p, output_filename=\"Service_Record_Dataset.csv\"):\n",
    "    \"\"\"\n",
    "    Orchestrates extraction with a rolling context to handle page-spanning records.\n",
    "    \"\"\"\n",
    "    all_rows = []\n",
    "    last_page_context = None\n",
    "\n",
    "    for i in range(start_p - 1, end_p):\n",
    "        print(f\"Processing Page {i+1}...\")\n",
    "        \n",
    "        # Pass the context from the previous iteration\n",
    "        rows = process_single_page(i, previous_context=last_page_context)\n",
    "        \n",
    "        if rows:\n",
    "            all_rows.extend(rows)\n",
    "            # Update the context to be the last entry of the current page\n",
    "            last_page_context = rows[-1] \n",
    "        \n",
    "    if not all_rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    \n",
    "    # Final CSV structure cleanup\n",
    "    required_cols = [\n",
    "        \"Full Name\", \"Educational Qualification\", \"Honorific/Title\", \"Date of Birth\", \n",
    "        \"Date of Joining Service\", \"Date of Arrival\", \"Voted/Non-voted\", \"Domicile\", \n",
    "        \"Station\", \"Substantive Appointment\", \"Subst. Date\", \"Officiating Appointment\", \"Off. Date\"\n",
    "    ]\n",
    "    df = df[required_cols] \n",
    "    \n",
    "    df.to_csv(os.path.join(BASE_DIR, output_filename), index=False)\n",
    "    print(f\"Extraction Complete. Saved to {output_filename}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56288aaf-393f-4df9-948d-83833a5f75bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Page 8...\n",
      "Extracted 147 lines of text.\n",
      "Processing Page 9...\n",
      "Extracted 177 lines of text.\n",
      "Extraction Complete. Saved to Page_8_9_ContextAware_Text_only.csv\n",
      "                       Full Name Educational Qualification Honorific/Title  \\\n",
      "0  Abraham, Edgar Garton Furtado                      B.A.       Oon. C.B.   \n",
      "1  Abraham, Edgar Garton Furtado                      B.A.       Oon. C.B.   \n",
      "2  Abraham, Edgar Garton Furtado                      B.A.       Oon. C.B.   \n",
      "3  Abraham, Edgar Garton Furtado                      B.A.       Oon. C.B.   \n",
      "4  Abraham, Edgar Garton Furtado                      B.A.       Oon. C.B.   \n",
      "\n",
      "  Date of Birth Date of Joining Service Date of Arrival Voted/Non-voted  \\\n",
      "0    08-04-1880              20-10-1904      28-11-1904       Non-voted   \n",
      "1    08-04-1880              20-10-1904      28-11-1904       Non-voted   \n",
      "2    08-04-1880              20-10-1904      28-11-1904       Non-voted   \n",
      "3    08-04-1880              20-10-1904      28-11-1904       Non-voted   \n",
      "4    08-04-1880              20-10-1904      28-11-1904       Non-voted   \n",
      "\n",
      "               Domicile     Station             Substantive Appointment  \\\n",
      "0  Non-Asiatic Domicile                                      Unattached   \n",
      "1  Non-Asiatic Domicile    Mianwali  A.C., 3rd grade (SettL. Train.ing)   \n",
      "2  Non-Asiatic Domicile     Sialkot  A.C., 3rd grade (Settl. Train.ing)   \n",
      "3  Non-Asiatic Domicile  Rawalpindi  A.C., 3rd grade (Settl. Train.ing)   \n",
      "4  Non-Asiatic Domicile      Ambala  A.C., 3rd grade (Settl. Train.ing)   \n",
      "\n",
      "  Subst. Date Officiating Appointment Off. Date  \n",
      "0  28-11-1904                    None      None  \n",
      "1  05-12-1904                    None      None  \n",
      "2  13-02-1905                    None      None  \n",
      "3  25-07-1905                    None      None  \n",
      "4  31-10-1905                    None      None  \n"
     ]
    }
   ],
   "source": [
    "test_results = run_extraction(start_p=8, end_p=9, output_filename=\"Page_8_9_ContextAware_Text_only.csv\")\n",
    "print(test_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2894911-c899-4480-8a2d-294dfe5f2e62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
